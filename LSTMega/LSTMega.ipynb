{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este projeto é um modelo de rede neural recorrente utilizando a arquitetura *Long Short-Term Memory* &ndash; LSTM para geração automatizada de textos.\n",
    "\n",
    "Antes de mais nada, é importante ressaltar que este projeto estava baseado em uma implementação da biblioteca [Keras](https://keras.io/), a partir de um [exemplo](https://keras.io/examples/lstm_text_generation/) da utilização de redes neurais recorrentes com LSTM, disponível na própria documentação do Keras.\n",
    "\n",
    "Porém, apesar dos vários ajustes no código-fonte, os resultados não estavam sendo satisfatórios. Então, para contornar esta situação, optou-se por utilizar outro algoritmo de geração de textos que, segundo a opinião de outros colegas, apresentava textos mais coesos e um melhor desempenho nos treinos.\n",
    "\n",
    "Este algoritmo, apesar de também utilizar algumas funções da biblioteca Keras, foi produzido pelo [TensorFlow](https://www.tensorflow.org/) e apresenta uma aplicação de rede neural recorrente para geração de textos utilizando a arquitetura GRU, podendo ser facilmente adaptada para a LSTM. Este exemplo pode ser acessado [aqui](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "No decorrer desta documentação, há alguns trechos de código de como este projeto estava antes de ser substituído pelo novo algoritmo. Todo o projeto e histórico de mudanças está disponível no repositório [lstmega](https://github.com/mlc2307/lstmega) no GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes da alteração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho da documentação será possível visualizar como a aplicação estava antes da troca de algoritmo.\n",
    "\n",
    "Para manter um pouco da organização deste *notebook*, optou-se por não documentar o trecho de código que fazia as importações do algoritmo anterior. Além do mais, os dois exemplos são bem semelhantes nesta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente, no primeiro bloco era informado o texto utilizado para o aprendizado do modelo, além de outras operações.\n",
    "\n",
    "```python\n",
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "\n",
    "path = get_file(basename(origin), origin)\n",
    "\n",
    "# Faz a leitura do arquivo, deixando todos os caracteres minúsculos.\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "print(\"Tamanho do arquivo:\", len(text))\n",
    "\n",
    "# Verifica quantos caracteres diferentes existem no texto.\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"Total de caracteres:\", len(chars))\n",
    "\n",
    "# Faz a indexação dos caracteres.\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, o algoritmo fatiava o texto em sequências semi-redundantes, cuja quantidade de caracteres destas sequências era definida pela variável `maxlen`.\n",
    "\n",
    "```python\n",
    "# Tamanho das fatias do texto.\n",
    "maxlen = 40\n",
    "\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Processo responsável por fatiar o texto em sequências.\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print(\"Total de sequências:\", len(sentences))\n",
    "\n",
    "print(\"\\nVetorizando as sequências...\")\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        \n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print(\"Sequências vetorizadas!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima etapa vinha a definição e construção do modelo de rede neural.\n",
    "\n",
    "```python\n",
    "print(\"Construindo o modelo...\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# A camada de entrada é do tipo LSTM.\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 24 neurônios e 0.2 de dropout.\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 32 neurônios e 0.3 de dropout.\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Camada de saída.\n",
    "model.add(Dense(len(chars), activation=\"softmax\"))\n",
    "\n",
    "print(\"Modelo construído!\\n\")\n",
    "\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo após, duas funções eram definidas para auxiliarem nos processos de treinamento do modelo. A função `sample` amostrava um índice de uma matriz de probabilidade e a função `on_epoch_end` mostrava um *feedback* com algumas informações ao final de cada época.\n",
    "\n",
    "```python\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print(\"\\n\\nGerando texto após época #{:d}...\".format(epoch + 1))\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"\\n{:->10s}\".format(\"\"))\n",
    "        print(\"Diversidade:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        \n",
    "        print(\"Gerando com a seed \\\"\" + sentence + \"\\\"\\n\")\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"{:s}{:->20s}\".format(\"\\n\" * 3, \"\"))\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E por último era feita a configuração da compilação e treinamento deste modelo, onde vários parâmtros haviam sido modificados em relação ao exemplo do original.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    "    optimizer=RMSprop()\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x, y,\n",
    "    batch_size=96,\n",
    "    epochs=64,\n",
    "    callbacks=[print_callback]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada época era gerado um texto usando diferentes *diversidades*. Quanto maior a diversidade, maior era a \"mistura\" de caracteres empregada na geração, e, quanto mais avançada a época, mais coeso era o texto.\n",
    "\n",
    "No final dos treinamentos, mesmo com vários ajustes, não eram obtidos bons resultados. Portanto, como já foi dito anteriormente, empregar um outro algoritmo pareceu uma solução interessante para este projeto.\n",
    "\n",
    "O novo algoritmo que substituiu o anterior é apresentado daqui em diante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparos iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca base deste projeto é o TensorFlow, mas também possui algumas funções principais trazidas da biblioteca Keras, além de outras bibliotecas mais comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deve estar na primeira linha.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Ao importar o TensoFlow, seriam exibidos alguns avisos. A função abaixo os ignora.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"tensorflow\", category=FutureWarning)\n",
    "\n",
    "# Caso este notebook esteja sendo executado no Google Colab...\n",
    "try:\n",
    "    # ... seleciona a versão do TensorFlow a ser importada a seguir.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Importa a biblioteca TensorFlow de fato.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ferramentas para manuseio de arquivos.\n",
    "from os.path import basename, join\n",
    "import io\n",
    "\n",
    "# Para a geração dos arrays a partir do texto.\n",
    "import numpy as np\n",
    "\n",
    "# Usado na geração aleatória do seed.\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Download* do *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mudar um pouco o modo como as coisas acontecem, o arquivo de texto usado como *dataset* será o mesmo usado antes da alteração. O texto está disponível remotamente e pode ser acessado através deste [link](https://s3.amazonaws.com/text-datasets/nietzsche.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset é o arquivo \"nietzsche.txt\", disponível em https://s3.amazonaws.com/text-datasets/nietzsche.txt\n"
     ]
    }
   ],
   "source": [
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "filename = basename(origin)\n",
    "\n",
    "# Retorna informações sobre o dataset.\n",
    "print(\"O dataset é o arquivo \\\"{}\\\", disponível em {}\".format(filename, origin))\n",
    "\n",
    "# Faz o download do dataset.\n",
    "path_to_file = tf.keras.utils.get_file(filename, origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lê o arquivo de texto e obtém algumas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O texto possui 600893 caracteres\n",
      "Existem 84 caracteres únicos\n"
     ]
    }
   ],
   "source": [
    "# Faz a leitura do arquivo de texto.\n",
    "text = io.open(path_to_file, encoding=\"utf-8\").read()\n",
    "\n",
    "# Quantidade de caracteres no texto.\n",
    "print(\"O texto possui {} caracteres\".format(len(text)))\n",
    "\n",
    "# Quantidade de caracteres únicos.\n",
    "vocab = sorted(set(text))\n",
    "print (\"Existem {} caracteres únicos\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir serão feitos alguns processamentos fundamentais com o texto do *dataset* baixado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de treinar o modelo, é necessário mapear as string para uma representação numérica. Para isso serão criadas duas tabelas indexadas: uma que mapeia caracteres à números, e outra números à caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento dos caracteres únicos à números.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do procedimento acima, cada caractere recebeu uma representação numérica. Abaixo há uma amostra para ilustrar isto de uma forma mais compreensível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I  s     t  h  e  r  e     n  o  t     g  r  o  u  n  d\n",
      "32 71  1 72 60 57 70 57  1 66 67 72  1 59 70 67 73 66 56\n"
     ]
    }
   ],
   "source": [
    "# Imprime uma linha de dados de largura fixa.\n",
    "def print_data(data, column_width=2):\n",
    "    for i in data[:-1]:\n",
    "        print(\"{:>{column_width}} \".format(i, column_width=column_width), end=\"\")\n",
    "    \n",
    "    print(\"{:>{column_width}}\".format(data[-1], column_width=column_width))\n",
    "\n",
    "# Início e tamanho da frase.\n",
    "phrase_begin = 54\n",
    "phrase_len = 19\n",
    "\n",
    "# Imprime os caracteres da frase e suas respectivas representações numéricas.\n",
    "print_data(text[phrase_begin:phrase_begin + phrase_len])\n",
    "print_data(text_as_int[phrase_begin:phrase_begin + phrase_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostras de treinamento e alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa o texto é dividido em sequências de amostragem. Cada sequência conterá `seq_length` caracteres do texto.\n",
    "\n",
    "Para cada sequência de entrada, a quantidade de caracteres das sequências alvo correspondentes será a mesma, mas com um deslocamento de um caractere para a esquerda.\n",
    "\n",
    "Isso quer dizer que os textos estão sendo dividos em partes de tamanho `seq_length + 1`. Por exemplo, suponha-se que `seq_length` é 4 e o trecho de texto é \"Tchau\". Neste caso, a sequência de entrada seria \"Tcha\", e a sequência alvo \"chau\".\n",
    "\n",
    "Para fazer isso, primeiro usa-se a função `tf.data.Dataset.from_tensor_slices` para converter o vetor de texto em uma indexação de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "R\n",
      "E\n",
      "F\n",
      "A\n",
      "C\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# O tamanho máximo de cada sequência.\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Cria as amostras de treinamento e os alvos.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(7):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `batch` permite converte facilmente estes caracteres avulsos em sequências de um tamanho específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all phi'\n",
      "'losophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible'\n",
      "'\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have '\n",
      "'been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself to b'\n",
      "'e won; and\\nat present every kind of dogma stands with sad and discouraged mien--IF,\\nindeed, it stands'\n",
      "' at all! For there are scoffers who maintain that it\\nhas fallen, that all dogma lies on the ground--n'\n",
      "'ay more, that it is at\\nits last gasp. But to speak seriously, there are good grounds for hoping\\nthat '\n"
     ]
    }
   ],
   "source": [
    "# Cria os batches para o treinamento.\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(7):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sequência é duplicada e deslocada para formar o texto de entrada e de destino usando o método `map` para aplicar uma simples função em cada *batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime a primeira sequência de amostragem e valores alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dado de entrada: 'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all ph'\n",
      "Dado alvo:       'REFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all phi'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print (\"Dado de entrada:\", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print (\"Dado alvo:      \", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada índice desses vetores é processado como uma etapa única. Para a entrada na etapa 0, o modelo recebe o índice para \"P\" e tenta prever o índice para \"R\" como o próximo caractere. No próximo passo, faz a mesma coisa, mas a RNN considera o contexto da etapa anterior além do caractere de entrada atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1:\n",
      "  Entrada: 39 ('P')\n",
      "  Saída esperada: 41 ('R')\n",
      "Passo 2:\n",
      "  Entrada: 41 ('R')\n",
      "  Saída esperada: 28 ('E')\n",
      "Passo 3:\n",
      "  Entrada: 28 ('E')\n",
      "  Saída esperada: 29 ('F')\n",
      "Passo 4:\n",
      "  Entrada: 29 ('F')\n",
      "  Saída esperada: 24 ('A')\n",
      "Passo 5:\n",
      "  Entrada: 24 ('A')\n",
      "  Saída esperada: 26 ('C')\n",
      "Passo 6:\n",
      "  Entrada: 26 ('C')\n",
      "  Saída esperada: 28 ('E')\n",
      "Passo 7:\n",
      "  Entrada: 28 ('E')\n",
      "  Saída esperada: 0 ('\\n')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:7], target_example[:7])):\n",
    "    print(\"Passo {}:\".format(i + 1))\n",
    "    print(\"  Entrada: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  Saída esperada: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batches* de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se `tf.data` para dividir o texto em sequências gerenciáveis. Mas antes de alimentar esses dados no modelo, é necessário embaralhar os dados e agrupá-los em *batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# Tamanho do batch.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tamanho do buffer para embaralhar o conjunto de dados.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo que será construído é do tipo `tf.keras.Sequential`.\n",
    "\n",
    "Para a construção deste modelo são utilizados três tipos de camadas:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: é a camada de entrada. Uma espécie de tabela que mapeará os números de cada caractere para um *array* com dimensões `embedding_dim`.\n",
    "* `tf.keras.layers.LSTM`: uma arquitetura de rede neural recorrente, com tamanho `units=rnn_units`.\n",
    "* `tf.keras.layers.Dense`: demais camadas e camada de saída, com `vocab_size` saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário, em caracteres.\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Dimensões da camada Embedding.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Quantidade de unidades da camada LSTM.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `build_model` é responsável pela instanciação de um modelo de rede neural, dados os seus parâmetros.\n",
    "\n",
    "Na primeira célula, seu algoritmo está igual ao do exemplo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrói um modelo dados os parâmetros.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    # Modelo do tipo sequencial.\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Adiciona uma camada de entrada do tipo Embedding.\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # Adiciona a camada LSTM.\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=True,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Última camada, do tipo Dense.\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resta agora instanciar um novo modelo usando a recém-criada função `build_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia um novo modelo.\n",
    "model = build_model(vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, o modelo é executado para verificar se tudo se comporta como o esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "sequence_length: 100\n",
      "vocab_size: 84\n"
     ]
    }
   ],
   "source": [
    "# Verifica o shape de saída do modelo.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    indexes_batch_predictions = (\"batch_size\", \"sequence_length\", \"vocab_size\")\n",
    "    \n",
    "    for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "        print(\"{}: {}\".format(i, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, o comprimento da sequência da entrada `sequence_length` é `100`, mas o modelo pode ser executado em entradas de qualquer comprimento, conforme célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter previsões reais do modelo, precisamos fazer amostras a partir da distribuição de saída, para obter índices reais de caracteres. Essa distribuição é definida pelos *logits* sobre o vocabulário dos caracteres.\n",
    "\n",
    "Nota: é importante fazer uma amostra dessa distribuição, pois o *argmax* da distribuição pode facilmente deixar o modelo preso em um *loop*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma amostragem da distribuição.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso dá, a cada passo, uma previsão do próximo índice de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39 23  2  8 40 58 24 20 48 44 28 10  4 48 80 30 45 20 74 15 57 26 31 81\n",
      " 64 70 63 36 50 31 51 58 64 39 70 71 36 59 68 49 48 58 77 24 31  2 78 30\n",
      " 60  7 78 51 81 37  4 64 48 64 38 76 22 12 18 59 42 53 77 53 51 72 42 59\n",
      " 57 39 45 82 20 77 28 67 39 33 38  0 66 25 31 36 37 63 37 52 51 81 71 18\n",
      " 12 30 72 13]\n"
     ]
    }
   ],
   "source": [
    "# Imprime os índices dos caracteres amostrados.\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao decodificar os índices é possível visualizar o texto previsto por este modelo que ainda não foi treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:\n",
      "  ' Schopenhauer\\'s Basis of Morality,\\ntranslated by Arthur B. Bullock, M.A. (1903).] \"the axiom about t'\n",
      "\n",
      "Predições para o próximo caractere:\n",
      "  \"P?!-QfA:YUE0'YäGV:v5eCHælrkM[H]flPrsMgpZYfyAH!zGh,z]æN'lYlOx=28gSaya]tSgePVé:yEoPJO\\nnBHMNkN_]æs82Gt3\"\n"
     ]
    }
   ],
   "source": [
    "# Decodifica os índices amostrados em sampled_indices.\n",
    "print(\"Entrada:\\n \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print(\"\\nPredições para o próximo caractere:\\n \", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, o problema pode ser tratado como um problema padrão de classificação. Dado o estado anterior da RNN e a entrada em tal intervalo de tempo, deve ser prevista, então, a classe do próximo caractere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizador e função de *loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função padrão de *loss* `tf.keras.losses.sparse_categorical_crossentropy` funciona muito bem neste caso porque ela é aplicada na última dimensão das predições.\n",
    "\n",
    "Nota: como o modelo retorna *logits*, é necessário definir a *flag* `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "sequence_length: 100\n",
      "vocab_size: 84\n",
      "scalar_loss: 4.4321527\n"
     ]
    }
   ],
   "source": [
    "# Função de loss.\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Aplica a função de loss no exemplo.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "    print(\"{}: {}\".format(i, e))\n",
    "\n",
    "print(\"scalar_loss:\", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é configurar o treinamento usando a função `tf.keras.Model.compile`.\n",
    "\n",
    "O exemplo original utiliza o otimizador `tf.keras.optimizers.Adam`, com seus argumentos padrão, e a função de *loss* definida anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o treinamento do modelo.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração dos *checkpoints*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se a função `tf.keras.callbacks.ModelCheckpoint` para garantir que os *checkpoints* sejam salvos durante o treinamento, conforme a célula a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde os checkpoints serão salvos.\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Nome dos arquivos dos checkpoints.\n",
    "checkpoint_prefix = join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Define o callback para cada época.\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                       save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima célula é definida a quantidade de épocas do treinamento.\n",
    "\n",
    "O exemplo original do TensorFlow define apenas 10 épocas para manter o treinamento mais razoável. Para este projeto, de início será definida uma quantidade de 20 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quantidade de épocas.\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `fit` do modelo é responsável por executar o treinamento. Após a execução ela retorna um histórico, que pode ser manipulado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 12s 129ms/step - loss: 2.9431\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 2.4328\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 2.2103\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 2.0421\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.9013\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.7866\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.6955\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.6216\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.5620\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.5134\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.4726\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.4375\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.4074\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.3808\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.3571\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.33571s\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.3159\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.2974\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 11s 118ms/step - loss: 1.2803\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.2632\n"
     ]
    }
   ],
   "source": [
    "# Executa o treinamento.\n",
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentemente do processo de geração de texto que o algoritmo anterior tinha, o exemplo original deste algoritmo executa a geração de texto apenas após todo o treinamento ser concluído, ou seja, o texto não é gerado a cada época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restauração do último *checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O *loop* de predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manter simples essa etapa de predição, será utilizado um tamanho de *batch* igual a 1.\n",
    "\n",
    "Devido à maneira como o estado da RNN é passado passo a passo, o modelo aceita apenas um tamanho fixo de *batch* depois de criado.\n",
    "\n",
    "Para executar o modelo com um `batch_size` diferente, é necessário reconstruí-lo e restaurar os pesos do último *checkpoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            21504     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstrói o modelo com um batch_size igual a 1.\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Carrega os pesos do último checkpoint.\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lógica do funcionamento da geração de texto é simples:\n",
    "\n",
    "* O algoritmo começa escolhendo uma sequência de início (*seed*), inicializando o estado RNN e definindo o número de caracteres a serem gerados.\n",
    "\n",
    "* Depois é obtida a distribuição de predição do próximo caractere usando a sequência de início e o estado RNN.\n",
    "\n",
    "* Em seguida, é usada uma distribuição categórica para calcular o índice do caractere previsto. Feito isto, este caractere passa a ser a próxima entrada do modelo.\n",
    "\n",
    "* O estado RNN retornado pelo modelo é realimentado no modelo para que agora tenha mais contexto, em vez de apenas uma palavra. Depois de prever a próxima palavra, os estados RNN modificados são novamente alimentados no modelo. É assim que a rede neural aprende à medida que obtém mais contexto das palavras previstas anteriormente.\n",
    "\n",
    "![Para gerar o texto, uma saída do modelo alimenta a entrada seguinte](images/text_generation_sampling.png)\n",
    "\n",
    "A função `generate_text` mostrada abaixo é responsável pela geração de texto e é utilizada no exemplo original.\n",
    "\n",
    "```python\n",
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, start_string):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Variável para armazenar os resultados.\n",
    "    text_generated = []\n",
    "\n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais surpreendente.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Redefine os estados do modelo para que tenha um batch_size igual à 1.\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # Remove a dimensão do batch.\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "        # juntamente com o estado oculto anterior.\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ampliar a gama de resultados deste projeto, tornou-se possível informar várias temperaturas para a função `generate_text`, conforme visto na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, start_string, temperatures):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Armazenará todos os textos gerados.\n",
    "    texts = []\n",
    "    \n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Redefine os estados do modelo para que tenha um batch_size igual à 1.\n",
    "    model.reset_states()\n",
    "    \n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais embaralhado.\n",
    "    for temperature in temperatures:\n",
    "        # Variável para armazenar os resultados.\n",
    "        text_generated = []\n",
    "        \n",
    "        # Faz a predição dos caracteres.\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "\n",
    "            # Remove a dimensão do batch.\n",
    "            predictions = tf.squeeze(predictions, 0) / temperature\n",
    "\n",
    "            # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "            # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "            # juntamente com o estado oculto anterior.\n",
    "            input_eval = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "        texts.append({\n",
    "            \"temperature\": temperature,\n",
    "            \"text_generated\": start_string + ''.join(text_generated)\n",
    "        })\n",
    "    \n",
    "    # Retorna todos os textos gerados.\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também foi desenvolvida a função `random_seed` para criar uma *seed* aleatória que será usada ao gerar o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(text, length):\n",
    "    # Obtém um começo aleatório para a seed.\n",
    "    start_index = random.randint(0, len(text) - length - 1)\n",
    "    \n",
    "    # Forma a seed aleatória a partir do caractere na posição\n",
    "    # start_index do text e de comprimento length.\n",
    "    return text[start_index:start_index + length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora basta chamar a função `generate_text` e informar seus parâmetros para que os textos sejam gerados. Além disso, a célula abaixo imprime os textos gerados com base nas temperaturas definidas na variável `temperatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando textos com a seed:\n",
      "\n",
      "\"y perceived that in the most personal considerations the most ge\"\n",
      "\n",
      "Temperatura: 0.5\n",
      "\n",
      "y perceived that in the most personal\n",
      "considerations the most general intellectual is the old strength, the most praise of anything betrays to be sure, and the spirituality, and he who does not a man whose strength, and all the world of involved. The contemplation of the subject and respect to the super-spiritual in the will to the senses of the spirit, and the slave and more\n",
      "delight in our present which is the present, however, that is the free spirit and the most distrust of all the whole senses and desires, and almost which he has the profound end for the German\n",
      "spirit and absolutely, which is the latter development that it is a whole defendence. Not the ear is almost every philosophy of the German superial things. The entire than anything of power, and the result of the commanding of any events which have been made upon the world of the struggle and hasing the same a man of the present experiences, the higher man loves all the strongest and present science which has hitherto\n",
      "been made a delicate contemplation of the anti-Saintifies and the shre\n",
      "--------------------\n",
      "Temperatura: 0.6\n",
      "\n",
      "y perceived that in the most personal\n",
      "considerations the most gend which is nothing either and perhaps as his man during more and the existence of its advantages of the profound spirituality, their duties or of the sublimates, or with it, the fixed of the Christian soul and the subject is the result\n",
      "for the problem of the coarseres flows and desires, as the master and an advance and restrainted and deniges the world with delive and\n",
      "the extent the one who has sith a things may be time and without realises the will the great sense of mounder, who beneved\n",
      "the European thinking is the same man, the man is to be above all the\n",
      "individual to the act of under the nature of the morality, the man who has disposed him, in his most dissinterestellects, and a constant of a manner, which some own self-contrary to\n",
      "the old such a\n",
      "haliged and so artists, that is not that it is\n",
      "not the fact that the individual and false of the history of the conduct that\n",
      "the rest and intellectual forms and enlightenments and the people it is for the most painful as love that which a\n",
      "--------------------\n",
      "Temperatura: 0.7\n",
      "\n",
      "y perceived that in the most personal\n",
      "considerations the most gell that the end is not independent, that is account, and\n",
      "it\n",
      "is almest as the hasys of any depression of the good bad and mestible to it respectance and habit to transfiern and above all that a dancere and intellectually, has its neither good. The result of the supers determined and books and forgetors must this case for the basis of all morality that the made of the presence of the exerciserve and\n",
      "its desirable that the will to its advantage--that is nowadays is the family individuals with it, which is the man of the God himself\n",
      "of contrarity that is the profounders, the master at first and bohe knowledge, in fact, revenge, nor had to the birror, the sentiment of the morality, by adarinary fail: the fear of his weaker must be abve all the contemples of the virtuous hand consequently, and vanity, a higher midest pleasure in his\n",
      "minder, he is possessed that this sur a rade, has been\n",
      "books, but no longer even in the same words.\n",
      "\n",
      "185. The last in contrary. But a\n",
      "predular\n",
      "religion of the su\n",
      "--------------------\n",
      "Temperatura: 0.8\n",
      "\n",
      "y perceived that in the most personal\n",
      "considerations the most gech appeals to the world, the \"spirituality is the blave may always generally because the perceive must after the heart, it is that is taken a bad capacity, to be indeverse or of indifferent quinement, and perhaps, and in its older and more understand the favour of the contrary, or else in the higher, the moral assertion in spite which have been find a realm will, and before a profound every a\n",
      "dangerous deed from nearly been spring itself: almost as a master and the last grandforrots. As the greater assumes of all the book, which is no\n",
      "stupidity,\n",
      "too order that renounce I have all renders a sort of all.\n",
      "\n",
      "257. (\"Well who find instinct even of a look of all souls.\n",
      "\n",
      "\n",
      "54\n",
      "\n",
      "=Vealthe repudiatin is a subsequently hard at all be honoured him owh as all the happeneant who can himself, as inspremervacity, the missure and things custom of endeality, which we will some\n",
      "conditions, who have their GREAD of the will, in whom the\n",
      "art for\n",
      "\"eherstood\" and usists its best, nave one who are so as his intent\n",
      "--------------------\n",
      "Temperatura: 0.9\n",
      "\n",
      "y perceived that in the most personal\n",
      "considerations the most geional willingly as well knows, their old meant of\n",
      "the dreadful\n",
      "spirit at midwally have been dust can be coolld independent completeing\n",
      "and words and delives)\n",
      "and groods and pity, how to be the world, and equally belong and bound \"God\"\n",
      "instincts and glorifications, and manifests, also,\n",
      "one have not deceived, boony; and are not inspiring late. A pleasure in every lixt, perhaps, his sword upon man for a looked upon the art\" it is the language as it is also in a\n",
      "world both human subvelized, does a man is bohe even where always distress. It distaken\n",
      "explanation evolved, and use him, after men of the\n",
      "subtlesty\n",
      "pestimary condected by the arces] and its sigh, and has\n",
      "not the belief in its injustice.\n",
      "And to be books! When one is\n",
      "not like them so\n",
      "that its have one beannd.\n",
      "\n",
      "872. What will that is almost\n",
      "every one is at fable, all this doubted, bring a gied, in the same and deducious fertine, and by means of openers, and in general these brants,\n",
      "whether it was slight, translate point of a holy add\n"
     ]
    }
   ],
   "source": [
    "# Cria uma seed aleatória.\n",
    "start_string = random_seed(text, BATCH_SIZE)\n",
    "\n",
    "print(\"Gerando textos com a seed:\\n\\n\\\"{}\\\"\\n\".format(start_string.replace(\"\\n\", \" \")))\n",
    "\n",
    "# Temperaturas que serão utilizadas na geração dos textos.\n",
    "temperatures = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Finalmente, executa a geração dos textos.\n",
    "texts = generate_text(model, start_string, temperatures)\n",
    "\n",
    "# Imprime os textos gerados.\n",
    "for t in texts:\n",
    "    print(\"Temperatura: {}\\n\\n{}\".format(t[\"temperature\"], t[\"text_generated\"]))\n",
    "    \n",
    "    # Imprime um separador entre os textos.\n",
    "    if t[\"temperature\"] != temperatures[-1]:\n",
    "        print(\"{:->20s}\".format(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esta ter sido a primeira execução do modelo, os textos, de certa forma, até impressionam pela sua coesão. Porém é interessante fazer algumas alterações para ver se é possível alcançar resultados melhores ainda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alterações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada alteração realizada, todo o trecho de código seguinte ao ponto alterado também é inserido junto à célula para ser reexecutado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira tentativa de melhoria que pode ser feita é em relação à temperatura. Optou-se por adicionar temperaturas mais altas e manter um intervalo maior entre elas. A *seed* aleatória também será alterada para aumentar a diversificação dos resultados.\n",
    "\n",
    "Entretanto, o modelo e outras variáveis do código permanecem os mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando textos com a seed:\n",
      "\n",
      "\"roblem of the value of truth presented itself before us--or was \"\n",
      "\n",
      "Temperatura: 0.5\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was æÆæææ8äÆæææëææVëæäæäëææææVëæäææäææääæææææææZëæääëææëææææäÆäææÆëëææëëæææÆææ(YæQææäæäÆëææææVæQëëæäääëÆæëÆæä(ëVææææXæææææäëæææÆææææäëæääææëæéæäæäæææäææææÆææÆææææææææææÆæÆææÆäëëëëëMXææææXÆææææææZäæZVë0æZ(=æ(ÆæVææææäææÆæÆëæääæææææäææVæäÆäëææäæäææææëëæææ(ÆæäææääæäæäÆæQäæÆææ=éææMæäææææMææ(ääææVëÆëëæææææäæZæäææVëëæäæëææääææÆææÆææäææææäæ7æäæääëæäæQäæääææäQäææ(ëëææëäæäÆÆæäæææëæQæææÆ(ëæäæææëäæÆæëäææëæææÆæÆæéVæäëææææëÆÆæVæææ(XæäëæäääëëëÆë(ÆæëææVææäæäæ(æææÆäæäëëäææææäæææéæëæææææZëÆæææZæéäææMÆææææäZääääæQÆÆæëäëææ(ææææÆäëGBëæäÆéæææëæëZææëææææÆæÆäæ(Æææëëëæææäæ(ÆëÆëæVæææææëQææææææäÆäææææëääëëäëææÆää(æææææQäæZ3æææÆæäääæææQæZÆXääææÆæAëëëæäæÆæ0ææëäææXÆæææææäæäëëäæäÆëäÆæääæææÆæÆææëQæZëææÆäMëæ(7æäæëäÆäæÆææææëææXæVxæææäæææææëææäëæææææææQääëæäæÆææÆæææJæVæææÆæææÆääæäMë(ææÆæ(ææÆææëæææäææææVææV0ææJææäÆæëæææVææ0äæëææææÆæææäQæQäæÆÆææëQææëHæääææææZÆææëWæææWæäëëæVëæææZëæ(ääæQæÆQææææäëääÆéëæææææææææææJæëæææÆÆææQäæëëææÆææææGæææëäæëDÆææéëæNäææææÆææQäææÆææææäæ0ëÆÆVëëZæææÆæææææÆXæëäæ(äÆæëæææ(äæææëæVææVæQëäëææVQææVææÆæäææ\n",
      "--------------------\n",
      "Temperatura: 0.7\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was æäääÆæäæäéëæXæDææäæäÆæäWæNæÆÆëjæQëÆäÆæXëQæÆææëDæäæJææZQææÆææææëYëæVëæä=ææäæææXæäæBæQë([ëÆæ[äæ(Æ0äææX4æäæë=ææææëææääæÆÆ(ë(æææææ(ëæææQZëëVäæäææææäëVëQäæ(ääæææÆæ(äæVæVëæXæææ(0äæææäææææLæäJæQää0ÆææäææäæëæéQææëä(ÆGææQÆHÆëVëGäææëæëæQëJMæææVÆäÆæ(æäæ0äæä0VæÆæææäëææVäææææäëXÆÆæDBæMë0XæäæææVëNCÆæäëZäëëäääBBQÆææä(ëæëëææQëææÆææ4æ0äÆëëMæäææXëææHëÆZQä(ëëQääæëäææéMëääÆæææäWææVëææææææÆäæ(æäCäæVæÆëÆ[ëÆäæ2æZææäææäæäCëÆæääéëæQ(æ[æVææÆZVVæÆëéæëÆæVæVæëJæä=ëæææææVëëæMÆææFëæÆæAäJæææäë=æææëäëææëHääæXæ[ääæäæQæäWæääëZëÆæëëäæëJæQäæVëWæ(ææBæäääWëäææÆæZæÆäRæCÆVëæZMæäææææëæääæææQääæé7æQæ(YÆëëMæ=æäMæäHÆæäæéæææVææææææQäæäXëXQëQéëëYÆÆQæÆäääPVæëæOäææææÆææææÆ0æææÆÆëäæL(ÆææææëXÆëëQëZ7äÆææëææ(ëéVÆæææGæVëæäææéæXæäæXëäÆäÆæëäÆÆäééääÆäÆÆæææÆææKæQÆëæææVæXäääXÆéææäæëææ(ë(ä(X(æææjZÆææäæ(=äëVææéëëæ0äJFGVë=ZäëëæQäXææVæëÆæäÆ=ëæéæQæÆäëæLæææäëCÆææëäëWæ(ääæXæZëëäëQäÆQææÆæXëMJ(ÆëVæXæäæ(æQëGNëæQæZæMBææäëLqäæäWææææÆæÆäéëæWæVæMÆæÆäXæVëæëëæ0ZVæä8äææ=Æææä7æë7VææææWæÆ(äæÆæäQæZëæëZæXæVæéCäæææäææéëÆBä0æä(ë(æëëVææXÆVëæëææääëææææææÆBæææææÆäQëäæW\n",
      "--------------------\n",
      "Temperatura: 0.9\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was VéëZäëæXZ(ææÆ(æJëæë=æVæ=æë0GææéëQkGæXZXæææ(ëëëäææNäëXWëéæäXBäëëWääZäQäÆäæÆëéææJQëæëéææéBäæM(]æMææëQäæYëZæëGæWëæÆJ4æÆ7ææ(QæVææJZææZYëë2Z3ääXMæ7Z9ZæH(æFZÆæXä(äæQæ(æQZéPé0GæX(éäéææ[äPUÆQëéæäæä0Ææé(ææëWÆææëä0æ7éQ!éVZXëæXJQæ(éääMæëäëXäëä=æëæ=ÆææGGKjäæææ(æëæëææÆæææææDæZææëQäæQë8äæRëëQæä2QQæÆæÆææ0ëQææÆZæW=QëææGä(=æäQjMëÆëæZæMäCëëëQëKVææHPBë8ZææDëXäÆÆæVëëYææä=äæLæXëëæÆVëæJÆÆææäëëXäZëëä(æææLZæÆHæä(QëKBæCjQëææææXæV?ææZäæÆäCéæYæææJäæ3äVjæÆææÆQÆLV[ææÆÆQæäZæäëä7Wæ(WæZÆÆäÆæVÆæVææææëäÆCæ1æ2Z2(;wZéMXéëJëDë=7X(æææéæëæOÆJæUæVææ3äëæäÆæ(ëëÆYÆäææ(ææëLäæÆ(4WÆääÆXäææMäëæææ[ëææQBZHæëéäæXëäÆZÆë(æÆæëÆæææXNG7Z(æëëX(äÆCVäÆ(æqæäMææææ(æQQæÆÆæäææM0CW(æWææQJæ00ÆæäëæVææä2æääJæVVkæäæææ=æXJëææHæMFëFæææææ(8æLÆ9æZë7ÆQëDæVTäääæë=ZææXæVæÆëäÆæQ(ääWææëZZÆæÆëMæäëëJæDææJHëäéæVZ(=(æææDæM(7ëæjMääæææëBäXæBëäHæëÆLæZÆéGëäæëQÆQææëëZZGæëVæQÆæÆæä(æææQéCÆæMQæWGäæëæUææLæëHä=æXGææVéMW2Qææ0ë2ææWæéXæZäææYYÆQëæVëéÆæ(æZäëZæMëæWæäëë7æ4ëæÆBéDäæäææÆääæZÆLZæäZÆVæÆæVæäæäæ0QääÆÆ(æ(jææëEäCQB=ææ7äëHæQææääÆJæäÆ(Æææëë8ææDæäëéHæäëë=äÆÆXZæëëæHéæäZëäÆëëDÆ\n",
      "--------------------\n",
      "Temperatura: 1.2\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was WëëëÆ=éëHZ4äæëNZÆæz7(ëV=7ëëQëZ(ëä=7ZFSëÆëSCéJæ6æBH4(ëVQBëMVäMéVWSæé1FæÆææä(QQäæëVÆjæ=ä3äÆZ[ëJIHææÆKJëVOä0PÆMëVBVææVZPëÆæXæVääIæÆÆMMæææ7CjFZÆPÆäXGZQQäæÆHæ(5Æ7HëéëCëTÆVëCZP3Æ0æIæZPVV3äæJ0VæAäëGéæææIDææÆæPæ(Më?ÆDBæ2VæëIëJ8BX(Ræ4TæYQääMFææëDzjHæZCQ(QXëZGæKQæÆZFHäWXæWæ_ë8Zæ\"ëæäæëëZZéZæä[äZëQXMQëëÆGBYZqQZVÆæQæFOÆ8æMæëëÆæXDZÆÆQ(PXæÆ(éZæ(ä=äYGäæÆëææëHæ83XZQäæZKéVÆäÆæäBÆ\"Gë]ëæ7JæÆëWæMÆææZ\"0æ2ëæUMëëëB=ZVæ2VëäÆZGëHLN1QD(XzëëQäRëÆéBäæ(ÆäÆQæ[ZQGWæQäDæGÆ(8Z95æ(éDææFéæVÆ(æVDëëëOä=ææÆGävKëéäQéëD(æBXæ(æÆÆQæ(ÆéæXëëJÆäXæëB2ææ?ZCWæZ0æäSæ(X3æXDBq9æä(]æVVæGäZæJæWG(æS(3ëSDäÆëYJæ2ÆëCVQJäZÆ3æäÆëHZæbZÆæÆëNëQä7ZWæZæÆWæQBä(MDJéëä5ëæäPÆéF4æQ;ÆZÆÆäWVæQVäVæëHæMéBæ?æ[Æä7ÆBæPëë2ëVGÆQÆZ8Z4äYLäæVCGXBHFääæXGëRVSMëY[äC0ä0ëæxZæKQäÆæFMjZë0æÆë7XDë0BMM0YëMäZMææQQææQæÆCæLCë9[æDÆ(C00ZKCæ(ëéBäZ(6æëæææ6X1ääPéXCÆäëëææ(äZz=ëæLææ0ÆäææYZæéZëÆXVÆQæZBëäLææVVxWäææ8éäÆëëZëëéæëVæQëNëQëVëäæä([ä(äQ(.ææz[ÆæëGæÆë0=æVZäNéMëëéZHFëëMäæXæÆWBW9Væ=]äÆæV7æÆÆéëÆæ2äHä2Q?ææ1?ëæëVäQæQæÆæAÆMB0éGëJGÆV3ääMVÆZäYéC=æDæëZ=Æ1äëZæÆæb=æëä((4ëUëä=ä7(6äQäæVMæ8JæVC\n",
      "--------------------\n",
      "Temperatura: 1.5\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was äZ2ZVzZKæSXæZ0ëÆDSÆWV5Qæë_Jäëä62æææÆ2ëæZFPææææÆSYDCëHjæäYæD770ZWäQëBMæëëQXæëGLæZZGéN=25äC3NëÆÆJÆZæëëUjHæMjæMÆæÆ(\"VMæëé(NÆW;3MÆVëäZéK3Z0æG2CéTæÆ(ë0Zæ_((3GææÆ00éäÆäææFF1ëÆÆ_1(HÆäCä;8ä)ZZGëëNAääLBDZ(?æ8HBæ3ëæëAZZë=ëM(ZæYäÆëYQx3(æDJ7Ææ)VæKäRæQxæë_(XæVë8æTëVæJëä6ëSæÆëäLæZDQxæQ;7GMQäÆWZYkCéMëZM:æëé8iæäSZä!æ9VQäéXéJBæL[æGÆXÆé(Æä(ææF8éëDæMÆ[éÆ5æäFqQZkëæVBëjMëVæVFFK(éUO(PqMjQ(=HÆääZZæVDæéQUDVfE1Zë7M=äMzZ1Æ4äB7QæVæë8PDæäZ_(ëQæäÆZëx=YX_F(XG20:L(xéLæææÆæCÆW,æUäEPVVSæÆXë4S_4GJéWgJLZä2Z[QWXÆWÆC=D3ëBZéëÆQ2x1æVvJ2éKVëææ(X6ÆëéUäææWäæBkæZæJæCZæMCéäZX0QQCæGZ(2JjëY6æW(YæëQäëBXææ0ëBRæëæEXæDÆëë_ä9Z=ëäææÆ8äjëäëëë7:(AæA(3æJCæ2CVjä7!ZæZæYæëäæYä(HZäÆHQÆäÆZé1QDë0Iä2PzjæZ(ëHëUDVäWëZ2MB1ëKd(=äæZTXëOäÆWëæëjQëFQQäëX]ä79[Jæää]éæé0?ÆäJZZ008æææ(Cë=ëVæZMÆäëJëæD7HÆ=ëKHæ1ä(æNgÆæ=æWëëM0ëWF_0'æC(Æ(m(MæDæëZMææC0äTMÆ0KGëU9X7PNäÆ6æVVëWæYDæ(Æ0æëXæCKZ'ZæWæäVKëYÆDæææëXJæL2æQæZ=PZ6X2ææZKC(7ëë)ä\"WW(Hæ0XB9Dæ0éSëVqmMëëæ(éZ3B=(=ä(Q'ZëäÆGæäJÆæäYÆBFä?Q00DæWä7(DQ9\"äëæ0(7J4B=VæäVQ;ëGäWÆQæzBQZ(ëëë=8æVÆXjJV[Qæ:WQQFæQæ[(ZNäæQÆ7ZëHIéëéYääWëéæ\n",
      "--------------------\n",
      "Temperatura: 2\n",
      "\n",
      "roblem of the\n",
      "value of truth presented itself before us--or was DOW=æ(?äQKæC=)([OAOÆVæFäWcæTæ(BAXëRF5Q(Pq_NUÆäQ5æBjV0æFBë9MXHjZææÆææXæD8J(äæ=(6æäOë6;æ7(2(ëëjMMQ92pJ2äG3G4QÆéë1äGäWCZGoSx[ÆA\"ëéÆÆQNéWkY5éM[=ÆëD8QXMHÆVæÆRX7()äæ3æWX(B=ææHJm7nSZxæW?(ÆäM,PB=jæ()KH2)9ZAjZJÆëë]PQÆ(æéMëJW1æMæ'Æ1äVëxæ(3Tä4GäéWääUXWQ0BWX_ë7væ6VQC=9O09HM2QZæHUZäj6æ)MGäQVéëägZäWBäLZMNQ0W4qæXMæy6QäA2éUæ0Lë0QkUJÆæÆ?YëëäViGGäxGLk0QS(æZä__BæÆæCæëqSG4FSZæ5LXä]0æÆ[ä1L3(jGZ3Eä60æFÆ]2ÆDÆ4jKæW(?BææIBV;=4(ÆééQæVXéV=kp2VyVæëzRJæFx2DCJ3Gä(=UæLæJÆMXéëæJDFjTCMMzÆëæAäVCJ3æBÆHäVääBÆæ[MDM(æRPäQMXMdH:Æ=0!j5ëæZ?Æ(QææQMzëIB=4NWHjÆë(]æJJG2ÆëHææMJB46YZZæR3(W8æqxS[(äGëC(WGS0W6DbQUBJæ=ZPæAäR6XT7YVëMKäQ0TRäëäPMV_ëjé(XS0MLhJë0äW?éFXæ7Jk2(NÆ0äæj(1éZææCæ2ä9Géääé[2bGÆjëHäM6M0M(éCz7æ1FQ9æVäææäéæäGBëMë?7äTG8ä7M0æjMRZææGé4XMHXæ7ëCé_é3GQäA02æXëëæUÆæG7æ6CJääIæ:ZGää=kÆZ.z,ZæÆ9(jX=qxVXÆ3QééÆ;ZHJm6[NÆéVZ(n0ëBÆæ??ë-ÆB\n",
      "tKÆD(ëÆLJ86ëzæäéWvHDÆ3Hæ[xæäQ(HéV3ëN_DæÆæqéäZéHZNXNKVæææVcæWéKXLZ6MDCNWB(éä_J2ëÆ8ZDCæJæëV;2éNWC0Iæj1JcvGCæJäUiæëGcFMGäXëFæJéZNDBDQC18oPVuë2XëVææ:ÆÆ?F=5FZXpG(äæ8CæDZNPäæJ7FzVææL(=æVjQBJBé_xGæBÆ0VPZé4ëææ\n"
     ]
    }
   ],
   "source": [
    "# Cria uma seed aleatória.\n",
    "start_string = random_seed(text, BATCH_SIZE)\n",
    "\n",
    "print(\"Gerando textos com a seed:\\n\\n\\\"{}\\\"\\n\".format(start_string.replace(\"\\n\", \" \")))\n",
    "\n",
    "# Temperaturas que serão utilizadas na geração dos textos.\n",
    "# Desta vez, temperaturas mais altas foram adicionadas e em um intervalo maior.\n",
    "temperatures = [0.5, 0.7, 0.9, 1.2, 1.5, 2]\n",
    "\n",
    "# Finalmente, executa a geração dos textos.\n",
    "texts = generate_text(model, start_string, temperatures)\n",
    "\n",
    "# Imprime os textos gerados.\n",
    "for t in texts:\n",
    "    print(\"Temperatura: {}\\n\\n{}\".format(t[\"temperature\"], t[\"text_generated\"]))\n",
    "    \n",
    "    # Imprime um separador entre os textos.\n",
    "    if t[\"temperature\"] != temperatures[-1]:\n",
    "        print(\"{:->20s}\".format(\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser claramente observável, os textos gerados, desta vez estão todos demasiadamente embaralhados, o que leva a acreditar que, dependendo da *seed* utilizada para gerar os textos, estes podem possuir um bom nível de coesão ou ficar totalmente incompreensíveis.\n",
    "\n",
    "Desta vez, a tentativa abaixo foi a de mudar a estrutura do modelo, entre outras variáveis:\n",
    "\n",
    "* `rnn_units` agora possui o dobro de seu antigo valor, passando a ser 2048.\n",
    "* `EPOCHS` passou para 50 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantidade de unidades da camada LSTM.\n",
    "rnn_units = 2048\n",
    "\n",
    "# Constrói um modelo dados os parâmetros.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    # Modelo do tipo sequencial.\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Adiciona uma camada de entrada do tipo Embedding.\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # Adiciona a camada LSTM.\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=True,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Última camada, do tipo Dense.\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Instancia um novo modelo.\n",
    "model = build_model(vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)\n",
    "\n",
    "# Verifica o shape de saída do modelo.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    indexes_batch_predictions = (\"batch_size\", \"sequence_length\", \"vocab_size\")\n",
    "    \n",
    "    for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "        print(\"{}: {}\".format(i, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma amostragem da distribuição.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "\n",
    "# Função de loss.\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Aplica a função de loss no exemplo.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "    print(\"{}: {}\".format(i, e))\n",
    "\n",
    "print(\"scalar_loss:\", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o treinamento do modelo.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss)\n",
    "\n",
    "# Define a quantidade de épocas.\n",
    "EPOCHS = 50\n",
    "\n",
    "# Executa o treinamento.\n",
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstrói o modelo com um batch_size igual a 1.\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Carrega os pesos do último checkpoint.\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma seed aleatória.\n",
    "start_string = random_seed(text, BATCH_SIZE)\n",
    "\n",
    "print(\"Gerando textos com a seed:\\n\\n\\\"{}\\\"\\n\".format(start_string.replace(\"\\n\", \" \")))\n",
    "\n",
    "# Temperaturas que serão utilizadas na geração dos textos.\n",
    "# Foram mantidas as últimas temperaturas utilizadas.\n",
    "temperatures = [0.5, 0.7, 0.9, 1.2, 1.5, 2]\n",
    "\n",
    "# Finalmente, executa a geração dos textos.\n",
    "texts = generate_text(model, start_string, temperatures)\n",
    "\n",
    "# Imprime os textos gerados.\n",
    "for t in texts:\n",
    "    print(\"Temperatura: {}\\n\\n{}\".format(t[\"temperature\"], t[\"text_generated\"]))\n",
    "    \n",
    "    # Imprime um separador entre os textos.\n",
    "    if t[\"temperature\"] != temperatures[-1]:\n",
    "        print(\"{:->20s}\".format(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
