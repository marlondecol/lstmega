{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este projeto é um modelo de rede neural recorrente utilizando a arquitetura *Long Short-Term Memory* &ndash; LSTM para geração automatizada de textos.\n",
    "\n",
    "Antes de mais nada, é importante ressaltar que este projeto estava baseado em uma implementação da biblioteca [Keras](https://keras.io/), a partir de um [exemplo](https://keras.io/examples/lstm_text_generation/) da utilização de redes neurais recorrentes com LSTM, disponível na própria documentação do Keras.\n",
    "\n",
    "Porém, apesar dos vários ajustes no código-fonte, os resultados não estavam sendo satisfatórios. Então, para contornar esta situação, optou-se por utilizar outro algoritmo de geração de textos que, segundo a opinião de outros colegas, apresentava textos mais coesos e um melhor desempenho nos treinos.\n",
    "\n",
    "Este algoritmo, apesar de também utilizar algumas funções da biblioteca Keras, foi produzido pelo [TensorFlow](https://www.tensorflow.org/) e apresenta uma aplicação de rede neural recorrente para geração de textos utilizando a arquitetura GRU, podendo ser facilmente adaptada para a LSTM. Este exemplo pode ser acessado [aqui](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "No decorrer desta documentação, há alguns trechos de código de como este projeto estava antes de ser substituído pelo novo algoritmo. Todo o projeto e histórico de mudanças está disponível no repositório [lstmega](https://github.com/mlc2307/lstmega) no GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes da alteração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho da documentação será possível visualizar como a aplicação estava antes da troca de algoritmo.\n",
    "\n",
    "Para manter um pouco da organização deste *notebook*, optou-se por não documentar o trecho de código que fazia as importações do algoritmo anterior. Além do mais, os dois exemplos são bem semelhantes nesta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente, no primeiro bloco era informado o texto utilizado para o aprendizado do modelo, além de outras operações.\n",
    "\n",
    "```python\n",
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "\n",
    "path = get_file(basename(origin), origin)\n",
    "\n",
    "# Faz a leitura do arquivo, deixando todos os caracteres minúsculos.\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "print(\"Tamanho do arquivo:\", len(text))\n",
    "\n",
    "# Verifica quantos caracteres diferentes existem no texto.\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"Total de caracteres:\", len(chars))\n",
    "\n",
    "# Faz a indexação dos caracteres.\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, o algoritmo fatiava o texto em sequências semi-redundantes, cuja quantidade de caracteres destas sequências era definida pela variável `maxlen`.\n",
    "\n",
    "```python\n",
    "# Tamanho das fatias do texto.\n",
    "maxlen = 40\n",
    "\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Processo responsável por fatiar o texto em sequências.\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print(\"Total de sequências:\", len(sentences))\n",
    "\n",
    "print(\"\\nVetorizando as sequências...\")\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        \n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print(\"Sequências vetorizadas!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima etapa vinha a definição e construção do modelo de rede neural.\n",
    "\n",
    "```python\n",
    "print(\"Construindo o modelo...\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# A camada de entrada é do tipo LSTM.\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 24 neurônios e 0.2 de dropout.\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 32 neurônios e 0.3 de dropout.\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Camada de saída.\n",
    "model.add(Dense(len(chars), activation=\"softmax\"))\n",
    "\n",
    "print(\"Modelo construído!\\n\")\n",
    "\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo após, duas funções eram definidas para auxiliarem nos processos de treinamento do modelo. A função `sample` amostrava um índice de uma matriz de probabilidade e a função `on_epoch_end` mostrava um *feedback* com algumas informações ao final de cada época.\n",
    "\n",
    "```python\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print(\"\\n\\nGerando texto após época #{:d}...\".format(epoch + 1))\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"\\n{:->10s}\".format(\"\"))\n",
    "        print(\"Diversidade:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        \n",
    "        print(\"Gerando com a seed \\\"\" + sentence + \"\\\"\\n\")\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"{:s}{:->20s}\".format(\"\\n\" * 3, \"\"))\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E por último era feita a configuração da compilação e treinamento deste modelo, onde vários parâmtros haviam sido modificados em relação ao exemplo do original.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    "    optimizer=RMSprop()\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x, y,\n",
    "    batch_size=96,\n",
    "    epochs=64,\n",
    "    callbacks=[print_callback]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada época era gerado um texto usando diferentes *diversidades*. Quanto maior a diversidade, maior era a \"mistura\" de caracteres empregada na geração, e, quanto mais avançada a época, mais coeso era o texto.\n",
    "\n",
    "No final dos treinamentos, mesmo com vários ajustes, não eram obtidos bons resultados. Portanto, como já foi dito anteriormente, empregar um outro algoritmo pareceu uma solução interessante para este projeto.\n",
    "\n",
    "O novo algoritmo que substituiu o anterior é apresentado daqui em diante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparos iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca base deste projeto é o TensorFlow, mas também possui algumas funções principais trazidas da biblioteca Keras, além de outras bibliotecas mais comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deve estar na primeira linha.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Ao importar o TensoFlow, seriam exibidos alguns avisos. A função abaixo os ignora.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"tensorflow\", category=FutureWarning)\n",
    "\n",
    "# Biblioteca base.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ferramentas para manuseio de arquivos.\n",
    "from os.path import basename, join\n",
    "import io\n",
    "\n",
    "# Para a geração dos arrays a partir do texto.\n",
    "import numpy as np\n",
    "\n",
    "# Usado na geração aleatória do seed.\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Download* do *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mudar um pouco o modo como as coisas acontecem, o arquivo de texto usado como *dataset* será o mesmo usado antes da alteração. O texto está disponível remotamente e pode ser acessado através deste [link](https://s3.amazonaws.com/text-datasets/nietzsche.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "filename = basename(origin)\n",
    "\n",
    "# Retorna informações sobre o dataset.\n",
    "print(\"O dataset é o arquivo \\\"{}\\\", disponível em {}\".format(filename, origin))\n",
    "\n",
    "# Faz o download do dataset.\n",
    "path_to_file = tf.keras.utils.get_file(filename, origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lê o arquivo de texto e obtém algumas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz a leitura do arquivo de texto.\n",
    "text = io.open(path_to_file, encoding=\"utf-8\").read()\n",
    "\n",
    "# Quantidade de caracteres no texto.\n",
    "print(\"O texto possui {} caracteres\".format(len(text)))\n",
    "\n",
    "# Quantidade de caracteres únicos.\n",
    "vocab = sorted(set(text))\n",
    "print (\"Existem {} caracteres únicos\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir serão feitos alguns processamentos fundamentais com o texto do *dataset* baixado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de treinar o modelo, é necessário mapear as string para uma representação numérica. Para isso serão criadas duas tabelas indexadas: uma que mapeia caracteres à números, e outra números à caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento dos caracteres únicos à números.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do procedimento acima, cada caractere recebeu uma representação numérica. Abaixo há uma amostra para ilustrar isto de uma forma mais compreensível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime uma linha de dados de largura fixa.\n",
    "def print_data(data, column_width=2):\n",
    "    for i in data[:-1]:\n",
    "        print(\"{:>{column_width}} \".format(i, column_width=column_width), end=\"\")\n",
    "    \n",
    "    print(\"{:>{column_width}}\".format(data[-1], column_width=column_width))\n",
    "\n",
    "# Início e tamanho da frase.\n",
    "phrase_begin = 54\n",
    "phrase_len = 19\n",
    "\n",
    "# Imprime os caracteres da frase e suas respectivas representações numéricas.\n",
    "print_data(text[phrase_begin:phrase_begin + phrase_len])\n",
    "print_data(text_as_int[phrase_begin:phrase_begin + phrase_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostras de treinamento e alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa o texto é dividido em sequências de amostragem. Cada sequência conterá `seq_length` caracteres do texto.\n",
    "\n",
    "Para cada sequência de entrada, a quantidade de caracteres das sequências alvo correspondentes será a mesma, mas com um deslocamento de um caractere para a esquerda.\n",
    "\n",
    "Isso quer dizer que os textos estão sendo dividos em partes de tamanho `seq_length + 1`. Por exemplo, suponha-se que `seq_length` é 4 e o trecho de texto é \"Tchau\". Neste caso, a sequência de entrada seria \"Tcha\", e a sequência alvo \"chau\".\n",
    "\n",
    "Para fazer isso, primeiro usa-se a função `tf.data.Dataset.from_tensor_slices` para converter o vetor de texto em uma indexação de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O tamanho máximo de cada sequência.\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Cria as amostras de treinamento e os alvos.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(7):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `batch` permite converte facilmente estes caracteres avulsos em sequências de um tamanho específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria os batches para o treinamento.\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(7):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sequência é duplicada e deslocada para formar o texto de entrada e de destino usando o método `map` para aplicar uma simples função em cada *batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime a primeira sequência de amostragem e valores alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print (\"Dado de entrada:\", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print (\"Dado alvo:      \", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada índice desses vetores é processado como uma etapa única. Para a entrada na etapa 0, o modelo recebe o índice para \"P\" e tenta prever o índice para \"R\" como o próximo caractere. No próximo passo, faz a mesma coisa, mas a RNN considera o contexto da etapa anterior além do caractere de entrada atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:7], target_example[:7])):\n",
    "    print(\"Passo {}:\".format(i + 1))\n",
    "    print(\"  Entrada: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  Saída esperada: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batches* de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se `tf.data` para dividir o texto em sequências gerenciáveis. Mas antes de alimentar esses dados no modelo, é necessário embaralhar os dados e agrupá-los em *batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do batch.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tamanho do buffer para embaralhar o conjunto de dados.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo que será construído é do tipo `tf.keras.Sequential`.\n",
    "\n",
    "Para a construção deste modelo são utilizados três tipos de camadas:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: é a camada de entrada. Uma espécie de tabela que mapeará os números de cada caractere para um *array* com dimensões `embedding_dim`.\n",
    "* `tf.keras.layers.LSTM`: uma arquitetura de rede neural recorrente, com tamanho `units=rnn_units`.\n",
    "* `tf.keras.layers.Dense`: demais camadas e camada de saída, com `vocab_size` saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário, em caracteres.\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Dimensões da camada Embedding.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Quantidade de unidades da camada LSTM.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `build_model` é responsável pela instanciação de um modelo de rede neural, dados os seus parâmetros.\n",
    "\n",
    "Na primeira célula, seu algoritmo está igual ao do exemplo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrói um modelo dados os parâmetros.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    # Modelo do tipo sequencial.\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Adiciona uma camada de entrada do tipo Embedding.\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # Adiciona a camada LSTM.\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=True,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Última camada, do tipo Dense.\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resta agora instanciar um novo modelo usando a recém-criada função `build_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia um novo modelo.\n",
    "model = build_model(vocab_size,\n",
    "                    embedding_dim,\n",
    "                    rnn_units,\n",
    "                    BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, o modelo é executado para verificar se tudo se comporta como o esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica o shape de saída do modelo.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    indexes_batch_predictions = (\"batch_size\", \"sequence_length\", \"vocab_size\")\n",
    "    \n",
    "    for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "        print(\"{}: {}\".format(i, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, o comprimento da sequência da entrada `sequence_length` é `100`, mas o modelo pode ser executado em entradas de qualquer comprimento, conforme célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter previsões reais do modelo, precisamos fazer amostras a partir da distribuição de saída, para obter índices reais de caracteres. Essa distribuição é definida pelos *logits* sobre o vocabulário dos caracteres.\n",
    "\n",
    "Nota: é importante fazer uma amostra dessa distribuição, pois o *argmax* da distribuição pode facilmente deixar o modelo preso em um *loop*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma amostragem da distribuição.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso dá, a cada passo, uma previsão do próximo índice de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprime os índices dos caracteres amostrados.\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao decodificar os índices é possível visualizar o texto previsto por este modelo que ainda não foi treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodifica os índices amostrados em sampled_indices.\n",
    "print(\"Entrada:\\n \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print(\"\\nPredições para o próximo caractere:\\n \", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, o problema pode ser tratado como um problema padrão de classificação. Dado o estado anterior da RNN e a entrada em tal intervalo de tempo, deve ser prevista, então, a classe do próximo caractere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizador e função de *loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função padrão de *loss* `tf.keras.losses.sparse_categorical_crossentropy` funciona muito bem neste caso porque ela é aplicada na última dimensão das predições.\n",
    "\n",
    "Nota: como o modelo retorna *logits*, é necessário definir a *flag* `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de loss.\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Aplica a função de loss no exemplo.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "    print(\"{}: {}\".format(i, e))\n",
    "\n",
    "print(\"scalar_loss:\", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é configurar o treinamento usando a função `tf.keras.Model.compile`.\n",
    "\n",
    "O exemplo original utiliza o otimizador `tf.keras.optimizers.Adam`, com seus argumentos padrão, e a função de *loss* definida anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o treinamento do modelo.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração dos *checkpoints*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se a função `tf.keras.callbacks.ModelCheckpoint` para garantir que os *checkpoints* sejam salvos durante o treinamento, conforme a célula a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde os checkpoints serão salvos.\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Nome dos arquivos dos checkpoints.\n",
    "checkpoint_prefix = join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Define o callback para cada época.\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                       save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima célula é definida a quantidade de épocas do treinamento.\n",
    "\n",
    "O exemplo original do TensorFlow define apenas 10 épocas para manter o treinamento mais razoável. Para este projeto, de início será definida uma quantidade de 20 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quantidade de épocas.\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `fit` do modelo é responsável por executar o treinamento. Após a execução ela retorna um histórico, que pode ser manipulado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executa o treinamento.\n",
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentemente do processo de geração de texto que o algoritmo anterior tinha, o exemplo original deste algoritmo executa a geração de texto apenas após todo o treinamento ser concluído, ou seja, o texto não é gerado a cada época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O *loop* de predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lógica do funcionamento da geração de texto é simples:\n",
    "\n",
    "* O algoritmo começa escolhendo uma sequência de início (*seed*), inicializando o estado RNN e definindo o número de caracteres a serem gerados.\n",
    "\n",
    "* Depois é obtida a distribuição de predição do próximo caractere usando a sequência de início e o estado RNN.\n",
    "\n",
    "* Em seguida, é usada uma distribuição categórica para calcular o índice do caractere previsto. Feito isto, este caractere passa a ser a próxima entrada do modelo.\n",
    "\n",
    "* O estado RNN retornado pelo modelo é realimentado no modelo para que agora tenha mais contexto, em vez de apenas uma palavra. Depois de prever a próxima palavra, os estados RNN modificados são novamente alimentados no modelo. É assim que a rede neural aprende à medida que obtém mais contexto das palavras previstas anteriormente.\n",
    "\n",
    "![Para gerar o texto, uma saída do modelo alimenta a entrada seguinte](images/text_generation_sampling.png)\n",
    "\n",
    "A função `generate_text` mostrada abaixo é responsável pela geração de texto e é utilizada no exemplo original.\n",
    "\n",
    "```python\n",
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, start_string):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Variável para armazenar os resultados.\n",
    "    text_generated = []\n",
    "\n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais surpreendente.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # No exemplo original, a função abaixo redefine os estados\n",
    "    # do modelo para que tenha um batch_size igual à 1.\n",
    "    # Isto não é utilizado neste projeto.\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # Remove a dimensão do batch.\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "        # juntamente com o estado oculto anterior.\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ampliar a gama de resultados deste projeto, algumas pequenas modificações foram feitas na função `generate_text` e podem ser visualizadas a seguir. Aliás, parte desta modificações são do algoritmo anterior e inclui, principalmente, *seeds* aleatórias e mais opções de temperaturas, além de outras funcionalidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, temperatures):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Armazenará todos os textos gerados.\n",
    "    texts = []\n",
    "    \n",
    "    # Obtém um começo aleatório para a seed.\n",
    "    start_index = random.randint(0, len(text) - seq_length - 1)\n",
    "    \n",
    "    # Forma a seed aleatória a partir do caractere na posição\n",
    "    # start_index do texto e de comprimento seq_length.\n",
    "    start_string = text[start_index:start_index + seq_length]\n",
    "    \n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "    \n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais embaralhado.\n",
    "    for temperature in temperatures:\n",
    "        # Variável para armazenar os resultados.\n",
    "        text_generated = []\n",
    "        \n",
    "        # Faz a predição dos caracteres.\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "\n",
    "            # Remove a dimensão do batch.\n",
    "            predictions = tf.squeeze(predictions, 0) / temperature\n",
    "\n",
    "            # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "            # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "            # juntamente com o estado oculto anterior.\n",
    "            input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "        texts.append({\n",
    "            \"temperature\": temperature,\n",
    "            \"text_generated\": start_string + ''.join(text_generated)\n",
    "        })\n",
    "    \n",
    "    # Retorna todos os textos gerados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora basta chamar a função `generate_text` e informar seus parâmetros para que os textos sejam gerados. Além disso, a célula abaixo imprime os textos gerados com base nas temperaturas definidas na variável `temperatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Temperaturas que serão utilizadas na geração dos textos.\n",
    "temperatures = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Finalmente, executa a geração dos textos.\n",
    "texts = generate_text(model, temperatures)\n",
    "\n",
    "# Imprime os textos gerados.\n",
    "for text in texts:\n",
    "    print(\"Temperatura: {}\\n\\n{}\".format(text[\"temperature\"], text[\"text_generated\"]))\n",
    "    \n",
    "    # Imprime um separador entre os textos.\n",
    "    if text[\"temperature\"] != temperatures[-1]:\n",
    "        print(\"{:->20s}\".format(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
