{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este projeto é um modelo de rede neural recorrente utilizando a arquitetura *Long Short-Term Memory* &ndash; LSTM para geração automatizada de textos.\n",
    "\n",
    "Antes de mais nada, é importante ressaltar que este projeto estava baseado em uma implementação da biblioteca [Keras](https://keras.io/), a partir de um [exemplo](https://keras.io/examples/lstm_text_generation/) da utilização de redes neurais recorrentes com LSTM, disponível na própria documentação do Keras.\n",
    "\n",
    "Porém, apesar dos vários ajustes no código-fonte, os resultados não estavam sendo satisfatórios. Então, para contornar esta situação, optou-se por utilizar outro algoritmo de geração de textos que, segundo a opinião de outros colegas, apresentava textos mais coesos e um melhor desempenho nos treinos.\n",
    "\n",
    "Este algoritmo, apesar de também utilizar algumas funções da biblioteca Keras, foi produzido pelo [TensorFlow](https://www.tensorflow.org/) e apresenta uma aplicação de rede neural recorrente para geração de textos utilizando a arquitetura GRU, podendo ser facilmente adaptada para a LSTM. Este exemplo pode ser acessado [aqui](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "No decorrer desta documentação, há alguns trechos de código de como este projeto estava antes de ser substituído pelo novo algoritmo. Todo o projeto e histórico de mudanças está disponível no repositório [lstmega](https://github.com/mlc2307/lstmega) no GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Antes da alteração"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trecho da documentação será possível visualizar como a aplicação estava antes da troca de algoritmo.\n",
    "\n",
    "Para manter um pouco da organização deste *notebook*, optou-se por não documentar o trecho de código que fazia as importações do algoritmo anterior. Além do mais, os dois exemplos são bem semelhantes nesta etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anteriormente, no primeiro bloco era informado o texto utilizado para o aprendizado do modelo, além de outras operações.\n",
    "\n",
    "```python\n",
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "\n",
    "path = get_file(basename(origin), origin)\n",
    "\n",
    "# Faz a leitura do arquivo, deixando todos os caracteres minúsculos.\n",
    "with io.open(path, encoding=\"utf-8\") as f:\n",
    "    text = f.read().lower()\n",
    "    \n",
    "print(\"Tamanho do arquivo:\", len(text))\n",
    "\n",
    "# Verifica quantos caracteres diferentes existem no texto.\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "print(\"Total de caracteres:\", len(chars))\n",
    "\n",
    "# Faz a indexação dos caracteres.\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, o algoritmo fatiava o texto em sequências semi-redundantes, cuja quantidade de caracteres destas sequências era definida pela variável `maxlen`.\n",
    "\n",
    "```python\n",
    "# Tamanho das fatias do texto.\n",
    "maxlen = 40\n",
    "\n",
    "step = 3\n",
    "\n",
    "sentences = []\n",
    "next_chars = []\n",
    "\n",
    "# Processo responsável por fatiar o texto em sequências.\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i:i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "\n",
    "print(\"Total de sequências:\", len(sentences))\n",
    "\n",
    "print(\"\\nVetorizando as sequências...\")\n",
    "\n",
    "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "        \n",
    "    y[i, char_indices[next_chars[i]]] = 1\n",
    "\n",
    "print(\"Sequências vetorizadas!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima etapa vinha a definição e construção do modelo de rede neural.\n",
    "\n",
    "```python\n",
    "print(\"Construindo o modelo...\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# A camada de entrada é do tipo LSTM.\n",
    "model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 24 neurônios e 0.2 de dropout.\n",
    "model.add(Dense(24, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Adiciona uma camada do tipo Dense com 32 neurônios e 0.3 de dropout.\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Camada de saída.\n",
    "model.add(Dense(len(chars), activation=\"softmax\"))\n",
    "\n",
    "print(\"Modelo construído!\\n\")\n",
    "\n",
    "model.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logo após, duas funções eram definidas para auxiliarem nos processos de treinamento do modelo. A função `sample` amostrava um índice de uma matriz de probabilidade e a função `on_epoch_end` mostrava um *feedback* com algumas informações ao final de cada época.\n",
    "\n",
    "```python\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype(\"float64\")\n",
    "    preds = np.log(preds) / temperature\n",
    "    \n",
    "    exp_preds = np.exp(preds)\n",
    "    \n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    \n",
    "    return np.argmax(probas)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    print(\"\\n\\nGerando texto após época #{:d}...\".format(epoch + 1))\n",
    "\n",
    "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "    \n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print(\"\\n{:->10s}\".format(\"\"))\n",
    "        print(\"Diversidade:\", diversity)\n",
    "\n",
    "        generated = \"\"\n",
    "        sentence = text[start_index: start_index + maxlen]\n",
    "        generated += sentence\n",
    "        \n",
    "        print(\"Gerando com a seed \\\"\" + sentence + \"\\\"\\n\")\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(400):\n",
    "            x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "            \n",
    "            for t, char in enumerate(sentence):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_char = indices_char[next_index]\n",
    "\n",
    "            generated += next_char\n",
    "            sentence = sentence[1:] + next_char\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"{:s}{:->20s}\".format(\"\\n\" * 3, \"\"))\n",
    "\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E por último era feita a configuração da compilação e treinamento deste modelo, onde vários parâmtros haviam sido modificados em relação ao exemplo do original.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy'],\n",
    "    optimizer=RMSprop()\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x, y,\n",
    "    batch_size=96,\n",
    "    epochs=64,\n",
    "    callbacks=[print_callback]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cada época era gerado um texto usando diferentes *diversidades*. Quanto maior a diversidade, maior era a \"mistura\" de caracteres empregada na geração, e, quanto mais avançada a época, mais coeso era o texto.\n",
    "\n",
    "No final dos treinamentos, mesmo com vários ajustes, não eram obtidos bons resultados. Portanto, como já foi dito anteriormente, empregar um outro algoritmo pareceu uma solução interessante para este projeto.\n",
    "\n",
    "O novo algoritmo que substituiu o anterior é apresentado daqui em diante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparos iniciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A biblioteca base deste projeto é o TensorFlow, mas também possui algumas funções principais trazidas da biblioteca Keras, além de outras bibliotecas mais comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deve estar na primeira linha.\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# Ao importar o TensoFlow, seriam exibidos alguns avisos. A função abaixo os ignora.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"tensorflow\", category=FutureWarning)\n",
    "\n",
    "# Caso este notebook esteja sendo executado no Google Colab...\n",
    "try:\n",
    "    # ... seleciona a versão do TensorFlow a ser importada a seguir.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Importa a biblioteca TensorFlow de fato.\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ferramentas para manuseio de arquivos.\n",
    "from os.path import basename, join\n",
    "import io\n",
    "\n",
    "# Para a geração dos arrays a partir do texto.\n",
    "import numpy as np\n",
    "\n",
    "# Usado na geração aleatória do seed.\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Download* do *dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mudar um pouco o modo como as coisas acontecem, o arquivo de texto usado como *dataset* será o mesmo usado antes da alteração. O texto está disponível remotamente e pode ser acessado através deste [link](https://s3.amazonaws.com/text-datasets/nietzsche.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O dataset é o arquivo \"nietzsche.txt\", disponível em https://s3.amazonaws.com/text-datasets/nietzsche.txt\n"
     ]
    }
   ],
   "source": [
    "# Origem remota do arquivo de texto utilizado no aprendizado do modelo.\n",
    "origin = \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
    "filename = basename(origin)\n",
    "\n",
    "# Retorna informações sobre o dataset.\n",
    "print(\"O dataset é o arquivo \\\"{}\\\", disponível em {}\".format(filename, origin))\n",
    "\n",
    "# Faz o download do dataset.\n",
    "path_to_file = tf.keras.utils.get_file(filename, origin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lê o arquivo de texto e obtém algumas informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O texto possui 600893 caracteres\n",
      "Existem 84 caracteres únicos\n"
     ]
    }
   ],
   "source": [
    "# Faz a leitura do arquivo de texto.\n",
    "text = io.open(path_to_file, encoding=\"utf-8\").read()\n",
    "\n",
    "# Quantidade de caracteres no texto.\n",
    "print(\"O texto possui {} caracteres\".format(len(text)))\n",
    "\n",
    "# Quantidade de caracteres únicos.\n",
    "vocab = sorted(set(text))\n",
    "print (\"Existem {} caracteres únicos\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processamento do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir serão feitos alguns processamentos fundamentais com o texto do *dataset* baixado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetorização do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de treinar o modelo, é necessário mapear as string para uma representação numérica. Para isso serão criadas duas tabelas indexadas: uma que mapeia caracteres à números, e outra números à caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapeamento dos caracteres únicos à números.\n",
    "char2idx = {u: i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois do procedimento acima, cada caractere recebeu uma representação numérica. Abaixo há uma amostra para ilustrar isto de uma forma mais compreensível."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I  s     t  h  e  r  e     n  o  t     g  r  o  u  n  d\n",
      "32 71  1 72 60 57 70 57  1 66 67 72  1 59 70 67 73 66 56\n"
     ]
    }
   ],
   "source": [
    "# Imprime uma linha de dados de largura fixa.\n",
    "def print_data(data, column_width=2):\n",
    "    for i in data[:-1]:\n",
    "        print(\"{:>{column_width}} \".format(i, column_width=column_width), end=\"\")\n",
    "    \n",
    "    print(\"{:>{column_width}}\".format(data[-1], column_width=column_width))\n",
    "\n",
    "# Início e tamanho da frase.\n",
    "phrase_begin = 54\n",
    "phrase_len = 19\n",
    "\n",
    "# Imprime os caracteres da frase e suas respectivas representações numéricas.\n",
    "print_data(text[phrase_begin:phrase_begin + phrase_len])\n",
    "print_data(text_as_int[phrase_begin:phrase_begin + phrase_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amostras de treinamento e alvos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa o texto é dividido em sequências de amostragem. Cada sequência conterá `seq_length` caracteres do texto.\n",
    "\n",
    "Para cada sequência de entrada, a quantidade de caracteres das sequências alvo correspondentes será a mesma, mas com um deslocamento de um caractere para a esquerda.\n",
    "\n",
    "Isso quer dizer que os textos estão sendo dividos em partes de tamanho `seq_length + 1`. Por exemplo, suponha-se que `seq_length` é 4 e o trecho de texto é \"Tchau\". Neste caso, a sequência de entrada seria \"Tcha\", e a sequência alvo \"chau\".\n",
    "\n",
    "Para fazer isso, primeiro usa-se a função `tf.data.Dataset.from_tensor_slices` para converter o vetor de texto em uma indexação de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P\n",
      "R\n",
      "E\n",
      "F\n",
      "A\n",
      "C\n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# O tamanho máximo de cada sequência.\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)\n",
    "\n",
    "# Cria as amostras de treinamento e os alvos.\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(7):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O método `batch` permite converte facilmente estes caracteres avulsos em sequências de um tamanho específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all phi'\n",
      "'losophers, in so far as they have been\\ndogmatists, have failed to understand women--that the terrible'\n",
      "'\\nseriousness and clumsy importunity with which they have usually paid\\ntheir addresses to Truth, have '\n",
      "'been unskilled and unseemly methods for\\nwinning a woman? Certainly she has never allowed herself to b'\n",
      "'e won; and\\nat present every kind of dogma stands with sad and discouraged mien--IF,\\nindeed, it stands'\n",
      "' at all! For there are scoffers who maintain that it\\nhas fallen, that all dogma lies on the ground--n'\n",
      "'ay more, that it is at\\nits last gasp. But to speak seriously, there are good grounds for hoping\\nthat '\n"
     ]
    }
   ],
   "source": [
    "# Cria os batches para o treinamento.\n",
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(7):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada sequência é duplicada e deslocada para formar o texto de entrada e de destino usando o método `map` para aplicar uma simples função em cada *batch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    \n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprime a primeira sequência de amostragem e valores alvo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dado de entrada: 'PREFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all ph'\n",
      "Dado alvo:       'REFACE\\n\\n\\nSUPPOSING that Truth is a woman--what then? Is there not ground\\nfor suspecting that all phi'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print (\"Dado de entrada:\", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print (\"Dado alvo:      \", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada índice desses vetores é processado como uma etapa única. Para a entrada na etapa 0, o modelo recebe o índice para \"P\" e tenta prever o índice para \"R\" como o próximo caractere. No próximo passo, faz a mesma coisa, mas a RNN considera o contexto da etapa anterior além do caractere de entrada atual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passo 1:\n",
      "  Entrada: 39 ('P')\n",
      "  Saída esperada: 41 ('R')\n",
      "Passo 2:\n",
      "  Entrada: 41 ('R')\n",
      "  Saída esperada: 28 ('E')\n",
      "Passo 3:\n",
      "  Entrada: 28 ('E')\n",
      "  Saída esperada: 29 ('F')\n",
      "Passo 4:\n",
      "  Entrada: 29 ('F')\n",
      "  Saída esperada: 24 ('A')\n",
      "Passo 5:\n",
      "  Entrada: 24 ('A')\n",
      "  Saída esperada: 26 ('C')\n",
      "Passo 6:\n",
      "  Entrada: 26 ('C')\n",
      "  Saída esperada: 28 ('E')\n",
      "Passo 7:\n",
      "  Entrada: 28 ('E')\n",
      "  Saída esperada: 0 ('\\n')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:7], target_example[:7])):\n",
    "    print(\"Passo {}:\".format(i + 1))\n",
    "    print(\"  Entrada: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  Saída esperada: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Batches* de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se `tf.data` para dividir o texto em sequências gerenciáveis. Mas antes de alimentar esses dados no modelo, é necessário embaralhar os dados e agrupá-los em *batches*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "# Tamanho do batch.\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tamanho do buffer para embaralhar o conjunto de dados.\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O modelo que será construído é do tipo `tf.keras.Sequential`.\n",
    "\n",
    "Para a construção deste modelo são utilizados três tipos de camadas:\n",
    "\n",
    "* `tf.keras.layers.Embedding`: é a camada de entrada. Uma espécie de tabela que mapeará os números de cada caractere para um *array* com dimensões `embedding_dim`.\n",
    "* `tf.keras.layers.LSTM`: uma arquitetura de rede neural recorrente, com tamanho `units=rnn_units`.\n",
    "* `tf.keras.layers.Dense`: demais camadas e camada de saída, com `vocab_size` saídas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamanho do vocabulário, em caracteres.\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Dimensões da camada Embedding.\n",
    "embedding_dim = 256\n",
    "\n",
    "# Quantidade de unidades da camada LSTM.\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `build_model` é responsável pela instanciação de um modelo de rede neural, dados os seus parâmetros.\n",
    "\n",
    "Na primeira célula, seu algoritmo está igual ao do exemplo original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constrói um modelo dados os parâmetros.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    # Modelo do tipo sequencial.\n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    # Adiciona uma camada de entrada do tipo Embedding.\n",
    "    model.add(tf.keras.layers.Embedding(vocab_size,\n",
    "                                        embedding_dim,\n",
    "                                        batch_input_shape=[batch_size, None]))\n",
    "    \n",
    "    # Adiciona a camada LSTM.\n",
    "    model.add(tf.keras.layers.LSTM(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   stateful=True,\n",
    "                                   recurrent_initializer='glorot_uniform'))\n",
    "    \n",
    "    # Última camada, do tipo Dense.\n",
    "    model.add(tf.keras.layers.Dense(vocab_size))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resta agora instanciar um novo modelo usando a recém-criada função `build_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instancia um novo modelo.\n",
    "model = build_model(vocab_size=vocab_size,\n",
    "                    embedding_dim=embedding_dim,\n",
    "                    rnn_units=rnn_units,\n",
    "                    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, o modelo é executado para verificar se tudo se comporta como o esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "sequence_length: 100\n",
      "vocab_size: 84\n"
     ]
    }
   ],
   "source": [
    "# Verifica o shape de saída do modelo.\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    indexes_batch_predictions = (\"batch_size\", \"sequence_length\", \"vocab_size\")\n",
    "    \n",
    "    for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "        print(\"{}: {}\".format(i, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No exemplo acima, o comprimento da sequência da entrada `sequence_length` é `100`, mas o modelo pode ser executado em entradas de qualquer comprimento, conforme célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           21504     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 84)            86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obter previsões reais do modelo, precisamos fazer amostras a partir da distribuição de saída, para obter índices reais de caracteres. Essa distribuição é definida pelos *logits* sobre o vocabulário dos caracteres.\n",
    "\n",
    "Nota: é importante fazer uma amostra dessa distribuição, pois o *argmax* da distribuição pode facilmente deixar o modelo preso em um *loop*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faz uma amostragem da distribuição.\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso dá, a cada passo, uma previsão do próximo índice de caracteres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6  2 67 68  1 47 41 22 34 14 61 17 57 27 27 57 21 60  6 44  6 41 78 71\n",
      "  3  5  3 69 76 65 55  9 49 76 41 17 68  0 72 58 38 63 16  0 43 12 56 78\n",
      " 14 50  8 35 17  9  9 53 18 53 82 21 34 60 32 63 56 68 31 78  6 16 72 41\n",
      " 40 46 81 39 58 48 67 81 56 49 29  9 54 42 18 36 18 10 62 39 79  9 17 74\n",
      " 73  8 33 63]\n"
     ]
    }
   ],
   "source": [
    "# Imprime os índices dos caracteres amostrados.\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao decodificar os índices é possível visualizar o texto previsto por este modelo que ainda não foi treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrada:\n",
      "  'and significance: in the\\nformer mere knowledge is sought and nothing else--whatever else be\\nincident'\n",
      "\n",
      "Predições para o próximo caractere:\n",
      "  ')!op XR=K4i7eDDe;h)U)Rzs\"(\"qxmc.ZxR7p\\ntfOk6\\nT2dz4[-L7..a8aé;KhIkdpHz)6tRQWæPfYoædZF.bS8M80jPÆ.7vu-Jk'\n"
     ]
    }
   ],
   "source": [
    "# Decodifica os índices amostrados em sampled_indices.\n",
    "print(\"Entrada:\\n \", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print(\"\\nPredições para o próximo caractere:\\n \", repr(\"\".join(idx2char[sampled_indices])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, o problema pode ser tratado como um problema padrão de classificação. Dado o estado anterior da RNN e a entrada em tal intervalo de tempo, deve ser prevista, então, a classe do próximo caractere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Otimizador e função de *loss*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função padrão de *loss* `tf.keras.losses.sparse_categorical_crossentropy` funciona muito bem neste caso porque ela é aplicada na última dimensão das predições.\n",
    "\n",
    "Nota: como o modelo retorna *logits*, é necessário definir a *flag* `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 64\n",
      "sequence_length: 100\n",
      "vocab_size: 84\n",
      "scalar_loss: 4.431426\n"
     ]
    }
   ],
   "source": [
    "# Função de loss.\n",
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Aplica a função de loss no exemplo.\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "\n",
    "for i, e in zip(indexes_batch_predictions, example_batch_predictions.shape):\n",
    "    print(\"{}: {}\".format(i, e))\n",
    "\n",
    "print(\"scalar_loss:\", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O próximo passo é configurar o treinamento usando a função `tf.keras.Model.compile`.\n",
    "\n",
    "O exemplo original utiliza o otimizador `tf.keras.optimizers.Adam`, com seus argumentos padrão, e a função de *loss* definida anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura o treinamento do modelo.\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuração dos *checkpoints*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utiliza-se a função `tf.keras.callbacks.ModelCheckpoint` para garantir que os *checkpoints* sejam salvos durante o treinamento, conforme a célula a seguir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diretório onde os checkpoints serão salvos.\n",
    "checkpoint_dir = './checkpoints'\n",
    "\n",
    "# Nome dos arquivos dos checkpoints.\n",
    "checkpoint_prefix = join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "# Define o callback para cada época.\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                       save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na próxima célula é definida a quantidade de épocas do treinamento.\n",
    "\n",
    "O exemplo original do TensorFlow define apenas 10 épocas para manter o treinamento mais razoável. Para este projeto, de início será definida uma quantidade de 20 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a quantidade de épocas.\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função `fit` do modelo é responsável por executar o treinamento. Após a execução ela retorna um histórico, que pode ser manipulado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "92/92 [==============================] - 11s 125ms/step - loss: 2.9221\n",
      "Epoch 2/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 2.3730\n",
      "Epoch 3/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 2.1164\n",
      "Epoch 4/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.9278\n",
      "Epoch 5/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.7814\n",
      "Epoch 6/20\n",
      "92/92 [==============================] - 11s 119ms/step - loss: 1.6694\n",
      "Epoch 7/20\n",
      "92/92 [==============================] - 11s 118ms/step - loss: 1.5849\n",
      "Epoch 8/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.5197\n",
      "Epoch 9/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.4671\n",
      "Epoch 10/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.4240\n",
      "Epoch 11/20\n",
      "92/92 [==============================] - 11s 114ms/step - loss: 1.3871\n",
      "Epoch 12/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 1.3542\n",
      "Epoch 13/20\n",
      "92/92 [==============================] - 11s 119ms/step - loss: 1.3244\n",
      "Epoch 14/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.2974\n",
      "Epoch 15/20\n",
      "92/92 [==============================] - 11s 115ms/step - loss: 1.2723\n",
      "Epoch 16/20\n",
      "92/92 [==============================] - 11s 114ms/step - loss: 1.2490\n",
      "Epoch 17/20\n",
      "92/92 [==============================] - 10s 113ms/step - loss: 1.2258\n",
      "Epoch 18/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.2001\n",
      "Epoch 19/20\n",
      "92/92 [==============================] - 10s 114ms/step - loss: 1.1764\n",
      "Epoch 20/20\n",
      "92/92 [==============================] - 11s 116ms/step - loss: 1.1557\n"
     ]
    }
   ],
   "source": [
    "# Executa o treinamento.\n",
    "history = model.fit(dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geração do texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentemente do processo de geração de texto que o algoritmo anterior tinha, o exemplo original deste algoritmo executa a geração de texto apenas após todo o treinamento ser concluído, ou seja, o texto não é gerado a cada época."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restauração do último *checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O *loop* de predição"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para manter simples essa etapa de predição, será utilizado um tamanho de *batch* igual a 1.\n",
    "\n",
    "Devido à maneira como o estado da RNN é passado passo a passo, o modelo aceita apenas um tamanho fixo de *batch* depois de criado.\n",
    "\n",
    "Para executar o modelo com um `batch_size` diferente, é necessário reconstruí-lo e restaurar os pesos do último *checkpoint*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            21504     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 84)             86100     \n",
      "=================================================================\n",
      "Total params: 5,354,580\n",
      "Trainable params: 5,354,580\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Reconstrói o modelo com um batch_size igual a 1.\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "# Carrega os pesos do último checkpoint.\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "# Exibe informações sobre o modelo.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lógica do funcionamento da geração de texto é simples:\n",
    "\n",
    "* O algoritmo começa escolhendo uma sequência de início (*seed*), inicializando o estado RNN e definindo o número de caracteres a serem gerados.\n",
    "\n",
    "* Depois é obtida a distribuição de predição do próximo caractere usando a sequência de início e o estado RNN.\n",
    "\n",
    "* Em seguida, é usada uma distribuição categórica para calcular o índice do caractere previsto. Feito isto, este caractere passa a ser a próxima entrada do modelo.\n",
    "\n",
    "* O estado RNN retornado pelo modelo é realimentado no modelo para que agora tenha mais contexto, em vez de apenas uma palavra. Depois de prever a próxima palavra, os estados RNN modificados são novamente alimentados no modelo. É assim que a rede neural aprende à medida que obtém mais contexto das palavras previstas anteriormente.\n",
    "\n",
    "![Para gerar o texto, uma saída do modelo alimenta a entrada seguinte](images/text_generation_sampling.png)\n",
    "\n",
    "A função `generate_text` mostrada abaixo é responsável pela geração de texto e é utilizada no exemplo original.\n",
    "\n",
    "```python\n",
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, start_string):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Variável para armazenar os resultados.\n",
    "    text_generated = []\n",
    "\n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais surpreendente.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Redefine os estados do modelo para que tenha um batch_size igual à 1.\n",
    "    model.reset_states()\n",
    "    \n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        # Remove a dimensão do batch.\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "        # juntamente com o estado oculto anterior.\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "    \n",
    "    return (start_string + ''.join(text_generated))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ampliar a gama de resultados deste projeto, tornou-se possível informar várias temperaturas para a função `generate_text`, conforme visto na célula abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gera um texto usando o modelo treinado.\n",
    "def generate_text(model, start_string, temperatures):\n",
    "    # Número de caracteres do texto.\n",
    "    num_generate = 1000\n",
    "    \n",
    "    # Armazenará todos os textos gerados.\n",
    "    texts = []\n",
    "    \n",
    "    # Vetoriza a seed, convertendo os caracteres para números.\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Redefine os estados do modelo para que tenha um batch_size igual à 1.\n",
    "    model.reset_states()\n",
    "    \n",
    "    # Baixas temperaturas resultam em um texto mais previsível.\n",
    "    # Temperaturas mais altas resultam em um texto mais embaralhado.\n",
    "    for temperature in temperatures:\n",
    "        # Variável para armazenar os resultados.\n",
    "        text_generated = []\n",
    "        \n",
    "        # Faz a predição dos caracteres.\n",
    "        for i in range(num_generate):\n",
    "            predictions = model(input_eval)\n",
    "\n",
    "            # Remove a dimensão do batch.\n",
    "            predictions = tf.squeeze(predictions, 0) / temperature\n",
    "\n",
    "            # Usa uma distribuição categórica para predizer a palavra retornada pelo modelo.\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "            # Realimenta a próxima entrada do modelo com a palavra prevista,\n",
    "            # juntamente com o estado oculto anterior.\n",
    "            input_eval = tf.expand_dims([predicted_id], 1)\n",
    "\n",
    "            text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "        texts.append({\n",
    "            \"temperature\": temperature,\n",
    "            \"text_generated\": start_string + ''.join(text_generated)\n",
    "        })\n",
    "    \n",
    "    # Retorna todos os textos gerados.\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Também foi desenvolvida a função `random_seed` para criar uma *seed* aleatória que será usada ao gerar o texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_seed(text, length):\n",
    "    # Obtém um começo aleatório para a seed.\n",
    "    start_index = random.randint(0, len(text) - length - 1)\n",
    "    \n",
    "    # Forma a seed aleatória a partir do caractere na posição\n",
    "    # start_index do text e de comprimento length.\n",
    "    return text[start_index:start_index + length]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora basta chamar a função `generate_text` e informar seus parâmetros para que os textos sejam gerados. Além disso, a célula abaixo imprime os textos gerados com base nas temperaturas definidas na variável `temperatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gerando textos com a seed:\n",
      "\n",
      "\"very grain of gold, long buried and imprisoned in mud and sand; \"\n",
      "\n",
      "Temperatura: 0.5\n",
      "\n",
      "very grain of gold, long buried and\n",
      "imprisoned in mud and sand; \"He must state after the universal century does not believe in the origin of the \"good\" of \"the results of the spirit and consequences the manifestation of the sense of the most indifferent and exceptional men, that it is a religious age, and in the more physician sense) believed that this is one of the future of the contemplation of the absolute domain of history of the world in deeper even in the belief in the common experience of shame, that in the extent of the answer and\n",
      "instruments of the powerful man, an inner power. The happiness of the present danger of the most end and man of the most extingual\n",
      "in Europe, it is a virtue of the moral philosophers and so far as the best of all philosophers and evil. And with everywhere it in the world in the relation of the laws of the end it is again and made our claim to the truth\n",
      "this man here a man of individual fact for its own interpretation and possible for the light of such a\n",
      "man of the moral philosophers has been made a period of think\n",
      "--------------------\n",
      "Temperatura: 0.6\n",
      "\n",
      "very grain of gold, long buried and\n",
      "imprisoned in mud and sand; ing in the circle of the\n",
      "states of his then be only the feeling of the state of life and in the secondary mad a community and with\n",
      "profound and still at langeance of the former and are in all protection in the result of a sign of others with a stronger, and a condition of sunflicence of\n",
      "eternal promises and self-existence in the longing strong and strive in his conscience the law gen-born of religious and similar that has a feeling of every decisive and religious feeling, and seems to make him without\n",
      "an exceptional with his folly of the pessimism, because in the present day away from strength he is no longer first one should remain the answer and the \"the sphere of human conception of the morality of \"the subjection\" in the contrary of\n",
      "morals, there is not at the bring and strongly, and also the ancient desire for his own\n",
      "blow truths when they are best so as we all that is at present to many, the most further designess of religious constitution, the expens his teacher and precisely in\n",
      "--------------------\n",
      "Temperatura: 0.7\n",
      "\n",
      "very grain of gold, long buried and\n",
      "imprisoned in mud and sand;  the individual\n",
      "works and plays belongs to the same form of society, and always himself only to repome this own\n",
      "personal and drinks the same\n",
      "time a sort of experience. It was a profound overorisund\n",
      "of retard to the body, and handshoud the\n",
      "animals, that there is no history of his delightenment and expression of himself and confessible that he is also a law being created, speaks an intellectual and discourable things that the fact that it is the individual conscience may be comes to all the sentiment of the antithesis of the \"philosopher is also essending the answer the should not have the state of conscience but as he that a cheriseness of his democrates and disposition that it is\n",
      "all other strength and hands on another and individual itself with generally and love, the type are according to the\n",
      "inferior operateness, and thereby civilization and self-consciously, and judgments in the religious life in the end becomes a\n",
      "different and cold in the command for the same mind and contrary\n",
      "in \n",
      "--------------------\n",
      "Temperatura: 0.8\n",
      "\n",
      "very grain of gold, long buried and\n",
      "imprisoned in mud and sand; fact the soul and many stimult\n",
      "of the person of the foreign,\n",
      "and therefore indistribut himself within the individual life there must be counter them a philosophy may be a religious ly them at lasts how BELIVIAVE in briefly the first them: they were experienced\n",
      "how an end feels an inner teached at any long as a conquest of the problem that infore this precisely at distrust and concealed with it, we are not miscourse and its readers all the\n",
      "opinion of art that mankind have not find us. Without how many struggle in\n",
      "order to be forgidated by the exchond and remains of which it being existed that will the desire of \"being.=--A world only a feeling of truth, however, while the influence of the contemplate men of the words of supposes it was first and discovering the essential epoth become without any old strength for an opinion of the sensations of feeling of psychological\n",
      "and means of men in the reduce of this THE DEED IN THE THE (SHAIST PHISOPPET ENTRUSI,\" Ind, and the synthesis of his rel\n",
      "--------------------\n",
      "Temperatura: 0.9\n",
      "\n",
      "very grain of gold, long buried and\n",
      "imprisoned in mud and sand; ations\n",
      "Our own eliminates him.\n",
      "\n",
      "\n",
      "\n",
      "HELIGARDES ONTER--HIS I believe in the spirit probably recupres himself approprist with a cernow learned at man, and account. A man HIS\n",
      "EERED-CONCEANDEN emball that is a man of inglibit and higher and are and called\n",
      "them, that there to seem the same\n",
      "prefermined and also more Englishmen \"T ATTED IS, PHTEMS BOUIE, the common ear!\n",
      "\n",
      "244. There is no longer false? considered \"T       Mo style, the foundation with an indignation, deliverybody; something\n",
      "uncertainty even expectant\" than a philosopher, that there actions wish to be ask!\"--He cannot existence in his stant\n",
      "and which apeas, does not thoroughly one for I\n",
      "my, and this is no deed of underspective mediocriting in the high-struggle and gregarious discoverory\n",
      "by means of the \"good\" and\n",
      "\"prahempthan mind,\" ages, thereby, among\n",
      "without servile, with all\n",
      "their passions, they \"the \"free spirit\" and especially his\n",
      "almost everything is the contrary; and with the most second circumstances, which one of the ch\n"
     ]
    }
   ],
   "source": [
    "# Cria uma seed aleatória.\n",
    "start_string = random_seed(text, BATCH_SIZE)\n",
    "\n",
    "print(\"Gerando textos com a seed:\\n\\n\\\"{}\\\"\\n\".format(start_string.replace(\"\\n\", \" \")))\n",
    "\n",
    "# Temperaturas que serão utilizadas na geração dos textos.\n",
    "temperatures = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "# Finalmente, executa a geração dos textos.\n",
    "texts = generate_text(model, start_string, temperatures)\n",
    "\n",
    "# Imprime os textos gerados.\n",
    "for text in texts:\n",
    "    print(\"Temperatura: {}\\n\\n{}\".format(text[\"temperature\"], text[\"text_generated\"]))\n",
    "    \n",
    "    # Imprime um separador entre os textos.\n",
    "    if text[\"temperature\"] != temperatures[-1]:\n",
    "        print(\"{:->20s}\".format(\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
